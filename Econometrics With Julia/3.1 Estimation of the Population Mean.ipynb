{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Estimation of the Population Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An estimator is a function of a sample of data to be drawn randomly from a population.\n",
    "- An estimate is the numerical value of the estimator when it is actually computed using data from a specific sample.\n",
    "- An estimator is a random variable because of randomness in selecting the sample, while an estimate is a nonrandom number.\n",
    "- Desirable characteristics of an estimator include unbiasedness, consistency and efficiency.\n",
    "\n",
    "\n",
    "- Unbiasedness:\n",
    "- The bias of $\\hat\\mu_Y$ is $E(\\hat\\mu_Y) - \\mu_Y$.\n",
    "- If the mean of the sampling distribution of some estimator $\\hat\\mu_Y$ for the population mean $\\mu_Y$ equals $\\mu_Y$, $E(\\hat\\mu_Y) = \\mu_Y$, then the estimator is unbiased for $\\mu_Y$.\n",
    "\n",
    "\n",
    "- Consistency: \n",
    "- We want the uncertainty of the estimator $\\mu_Y$ to decrease as the number of observations in the sample grows.\n",
    "- More precisely, we want the probability that the estimate $\\hat\\mu_Y$ falls within a small interval around the true value $\\mu_Y$ to get increasingly closer to 1 as n grows; i.e we want the estimator to be consistent with the true value.\n",
    "- $\\hat\\mu_Y$ is a consistent estimator of $\\mu_Y$ if $\\hat\\mu_Y \\xrightarrow{p} \\mu_Y$, or more precisely: \n",
    "\n",
    "\\begin{equation}\n",
    "P(|\\hat{\\mu} - \\mu|<\\epsilon) \\xrightarrow[n \\rightarrow \\infty]{p} 1 \\quad \\text{for any}\\quad\\epsilon>0\n",
    "\\end{equation}\n",
    "\n",
    "- This expression says that the probability of observing a deviation from the true value $\\mu$ that is smaller than some arbitrary $\\epsilon > 0$ converges to 1 as n grows (note that consistency does not require unbiasedness).\n",
    "\n",
    "\n",
    "- Efficiency:\n",
    "- Suppose we have two estimators, $\\hat\\mu_Y$ and $\\tilde\\mu_Y$ and for some given sample size n it holds that $E(\\hat\\mu_Y) = E(\\overset{\\sim}{\\mu}_Y) = \\mu_Y$ (both are unbiased).\n",
    "- If $\\text{Var}(\\hat\\mu_Y) < \\text{Var}(\\overset{\\sim}{\\mu}_Y)$ then we then prefer to use $\\hat\\mu_Y$ as it has a lower variance than $\\tilde\\mu_Y$, meaning that $\\hat\\mu_Y$ is more efficient in using the information provided by the observations in the sample.\n",
    "\n",
    "\n",
    "- When carrying out statistical inference, we assume there is some underlying distribution from which we are sampling.\n",
    "- Importantly, we assume that each observation is statistically independent and identically distributed.\n",
    "- That is, from a probabilistic perspective, the observations are taken as independent and identically distributed (i.i.d.) random variables to form a random sample.\n",
    "- Typically, we use estimators to compute estimates from the random sample, which can be modelled as random variables.\n",
    "- Two common estimators include the sample mean $\\overline{Y}$ and sample variance $S^2$, which are used to compute the population mean $\\mu_Y$ and population variance $\\sigma^2_Y$ estimates.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\overline{Y} = \\frac{1}{n} \\sum_{i=1}^n Y_i \\\\\n",
    "S^2 = \\frac{1}{n - 1} \\sum_{i=1}^n (Y_i - \\overline{Y})^2\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- As shown in section 2.5, $E(\\overline{Y}) = \\mu_Y$, so $\\overline{Y}$ is an unbiased estimator of $\\mu_Y$.\n",
    "- Similarly, the law of large numbers states that $\\hat\\mu_Y \\xrightarrow{p} \\mu_Y$; that is, $\\overline{Y}$ is consistent.\n",
    "- $\\overline{Y}$ is the most efficient estimator of all unbiased estimators that are weighted averages of $Y_1,\\dots,Y_n$.\n",
    "- Said differently, $\\overline{Y}$ is the Best Linear Unbiased Estimator (BLUE); that is, it is the most efficient (best) estimator among all estimators that are unbiased and are linear functions of $Y_1,\\dots,Y_n$.\n",
    "\n",
    "\n",
    "- Through a derivation using algebra/calculus, it can be shown that the sample average $\\overline{Y}$ also provides the best fit to the data in the sense that the average squared differences between the observations and $\\overline{Y}$ are the smallest of all possible estimators.\n",
    "- That is, $\\overline{Y}$ is the least squares estimator of $\\mu_Y$, it is the estimator m that minimizes the least squares problem of $\\sum^{n}_{i = 1}(Y_i - m)^2$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
